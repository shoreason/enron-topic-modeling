{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10348, 2)\n",
      "                              file  \\\n",
      "186822  jones-t/all_documents/634.   \n",
      "308790  mann-k/all_documents/5690.   \n",
      "82383         dasovich-j/sent/423.   \n",
      "227299          kaminski-v/var/63.   \n",
      "301824     mann-k/_sent_mail/3208.   \n",
      "\n",
      "                                                  message  \n",
      "186822  Message-ID: <17820178.1075846925335.JavaMail.e...  \n",
      "308790  Message-ID: <29110382.1075845717882.JavaMail.e...  \n",
      "82383   Message-ID: <6812040.1075843194135.JavaMail.ev...  \n",
      "227299  Message-ID: <21547648.1075856642126.JavaMail.e...  \n",
      "301824  Message-ID: <12684200.1075846107179.JavaMail.e...  \n"
     ]
    }
   ],
   "source": [
    "emails = pd.read_csv('emails.csv')\n",
    "# email_subset = emails[:10000]\n",
    "email_subset = emails.sample(frac=0.02, random_state=1)\n",
    "\n",
    "print(email_subset.shape)\n",
    "print(email_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_message(raw_message):\n",
    "    lines = raw_message.split('\\n')\n",
    "    email = {}\n",
    "    message = ''\n",
    "    keys_to_extract = ['from', 'to']\n",
    "    for line in lines:\n",
    "        if ':' not in line:\n",
    "            message += line.strip()\n",
    "            email['body'] = message\n",
    "        else:\n",
    "            pairs = line.split(':')\n",
    "            key = pairs[0].lower()\n",
    "            val = pairs[1].strip()\n",
    "            if key in keys_to_extract:\n",
    "                email[key] = val\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_into_emails(messages):\n",
    "    emails = [parse_raw_message(message) for message in messages]\n",
    "    return {\n",
    "        'body': map_to_list(emails, 'body'),\n",
    "        'to': map_to_list(emails, 'to'),\n",
    "        'from_': map_to_list(emails, 'from')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_list(emails, key):\n",
    "    results = []\n",
    "    for email in emails:\n",
    "        if key not in email:\n",
    "            results.append('')\n",
    "        else:\n",
    "            results.append(email[key])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body  \\\n",
      "0  It would be nice if you could be at my dinner,...   \n",
      "1  Absolutely.Good point!  Can Peter start to dra...   \n",
      "2  My apologies.  My schedule melted down after w...   \n",
      "3  Vince,UK VAR breached the limit last week.UK t...   \n",
      "4  Any problems/comments?AM ---------------------...   \n",
      "\n",
      "                                                  to  \\\n",
      "0                           alicia.goodrow@enron.com   \n",
      "1                          Kay Mann/Corp/Enron@ENRON   \n",
      "2                        christine.piesco@oracle.com   \n",
      "3  Richard Lewis/LON/ECT@ECT, James New/LON/ECT@E...   \n",
      "4  Don Hammond/PDX/ECT@ECT, Jody Blackburn/PDX/EC...   \n",
      "\n",
      "                               from_  \n",
      "0               tana.jones@enron.com  \n",
      "1  Sheila Tweed@ECT on 05/15/2001 06  \n",
      "2            jeff.dasovich@enron.com  \n",
      "3        tanya.tamarchenko@enron.com  \n",
      "4                 kay.mann@enron.com  \n"
     ]
    }
   ],
   "source": [
    "email_df = pd.DataFrame(parse_into_emails(email_subset.message))\n",
    "print(email_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  >>> import nltk\n",
    "#  >>> nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n"
     ]
    }
   ],
   "source": [
    "# prep NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My apologies.  My schedule melted down after we talked on Monday.  Here'swhere folks came out.  There's some concern about size.  We're supposed to beno larger than 3, but we lobbied Aceves and he apparently Ok'd our\"oversized\" group.  The other folks in the group--who talked to himoriginally--are pretty sure that five will violate the rules.  Folks wonderedif there were other groups that are smaller than ours that you could hook upwith.  Sorry about that---it's a wrinkle that I didn't think about when wespoke.  If it gets real ugly trying to find a smaller group, let me know.Fortunately there's not another team case due for two weeks.Best,Jeff\n"
     ]
    }
   ],
   "source": [
    "print(email_df.iloc[2]['body']) # displays info below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert email body to list\n",
    "data = email_df.body.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize - break down each sentence into a list of words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vince', 'uk', 'var', 'breached', 'the', 'limit', 'last', 'week', 'uk', 'traders', 'asked', 'us', 'to', 'review', 'the', 'correlations', 'across', 'uk', 'gas', 'and', 'power', 'aswell', 'as', 'the', 'correlations', 'across', 'efa', 'slots', 'we', 'did', 'part', 'of', 'the', 'work', 'last', 'week', 'now', 'we', 'll', 'update', 'the', 'correlations', 'based', 'on', 'historical', 'prices', 'tanya', 'richard', 'lewisleppard', 'lon', 'ect', 'ect', 'rudy', 'dautel', 'hou', 'ect', 'ect', 'kirstee', 'hewitt', 'lon', 'ect', 'ect', 'naveen', 'andrews', 'corp', 'enron', 'enron', 'david', 'port', 'market', 'risk', 'corp', 'enron', 'enron', 'tedmurphy', 'hou', 'ect', 'ect', 'simon', 'hastings', 'lon', 'ect', 'ect', 'paul', 'arcy', 'lon', 'ect', 'ect', 'amirghodsian', 'lon', 'ect', 'ectthanks', 'tanya', 'these', 'are', 'interesting', 'results', 'am', 'on', 'vacation', 'next', 'week', 'sohere', 'are', 'my', 'current', 'thoughts', 'am', 'contactable', 'on', 'my', 'mobile', 'if', 'necessary', 'gas', 'to', 'power', 'correlationsi', 'see', 'your', 'point', 'about', 'gas', 'to', 'power', 'correlation', 'only', 'affecting', 'var', 'for', 'theconservative', 'long', 'term', 'correlation', 'combined', 'var', 'is', 'mm', 'less', 'thanpreviously', 'expected', 'so', 'how', 'does', 'this', 'affect', 'the', 'limit', 'breach', 'we', 'are', 'still', 'over', 'our', 'uk', 'power', 'limit', 'but', 'the', 'limit', 'was', 'set', 'when', 'wewere', 'assuming', 'no', 'gas', 'power', 'correlation', 'and', 'therefore', 'higher', 'portfolio', 'var', 'suggested', 'way', 'forward', 'given', 'the', 'importance', 'of', 'the', 'spread', 'options', 'to', 'the', 'ukgas', 'and', 'power', 'books', 'can', 'we', 'allocate', 'to', 'the', 'gas', 'and', 'power', 'books', 'share', 'of', 'the', 'reduction', 'inportfolio', 'var', 'ie', 'reduction', 'portfolio', 'var', 'sum', 'power', 'var', 'gas', 'var', 'also', 'if', 'understand', 'your', 'mail', 'correctly', 'matrix', 'implies', 'gas', 'is', 'consistent', 'with', 'our', 'correlation', 'curves', 'and', 'this', 'reduces', 'totalvar', 'by', 'mm', 'efa', 'slot', 'correlationsthe', 'issue', 'of', 'whether', 'our', 'existing', 'efa', 'to', 'efa', 'correlation', 'matrix', 'is', 'correct', 'isa', 'separate', 'issue', 'don', 'understand', 'where', 'the', 'matrix', 'efa', 'to', 'efacorrelations', 'come', 'from', 'but', 'am', 'happy', 'for', 'you', 'to', 'run', 'some', 'from', 'the', 'forward', 'curves', 'use', 'the', 'first', 'years', 'wouldsuggest', 'our', 'original', 'matrix', 'was', 'based', 'on', 'historicals', 'but', 'the', 'analysis', 'isworth', 'doing', 'again', 'your', 'matrix', 'results', 'certainly', 'indicate', 'how', 'importantthese', 'correlations', 'are', 'closing', 'thoughtsfriday', 'trading', 'left', 'us', 'longer', 'so', 'would', 'not', 'expect', 'limit', 'breach', 'onmonday', 'we', 'are', 'still', 'reviewing', 'the', 'shape', 'of', 'the', 'long', 'term', 'curve', 'and', 'dlike', 'to', 'wait', 'until', 'both', 'simon', 'hastings', 'and', 'are', 'back', 'in', 'the', 'office', 'mondayweek', 'before', 'finalising', 'this', 'tamarchenkonew', 'lon', 'ect', 'ect', 'steven', 'leppard', 'lon', 'ect', 'ect', 'rudy', 'dautel', 'hou', 'ect', 'ect', 'kirsteehewitt', 'lon', 'ect', 'ect', 'naveen', 'andrews', 'corp', 'enron', 'enron', 'david', 'port', 'marketrisk', 'corp', 'enron', 'enron', 'ted', 'murphy', 'hou', 'ect', 'ecteverybody', 'oliver', 'sent', 'us', 'the', 'var', 'number', 'for', 'different', 'correlations', 'for', 'uk', 'powerportfolio', 'separately', 'from', 'uk', 'gas', 'portfolio', 'first', 'if', 'var', 'is', 'calculated', 'accurately', 'the', 'correlation', 'between', 'power', 'and', 'gascurves', 'should', 'not', 'affect', 'var', 'number', 'for', 'power', 'and', 'var', 'number', 'for', 'gas', 'onlythe', 'aggregate', 'number', 'will', 'be', 'affected', 'the', 'changes', 'you', 'see', 'are', 'due', 'to', 'thefact', 'that', 'we', 'use', 'monte', 'carlo', 'simulation', 'method', 'which', 'accuracy', 'depends', 'on', 'the', 'number', 'of', 'simulations', 'even', 'if', 'we', 'don', 'changethe', 'correlations', 'but', 'use', 'different', 'realizations', 'of', 'random', 'numbers', 'we', 'get', 'slightly', 'different', 'result', 'from', 'the', 'model', 'we', 'should', 'look', 'at', 'the', 'aggregate', 'number', 'calculated', 'weighted', 'correlations', 'based', 'on', 'curves', 'got', 'from', 'paul', 'as', 'theweights', 'along', 'the', 'term', 'structure', 'used', 'the', 'product', 'of', 'price', 'position', 'andvolatility', 'for', 'each', 'time', 'bucket', 'for', 'gas', 'and', 'each', 'of', 'efa', 'slots', 'the', 'these', 'numbers', 'into', 'the', 'original', 'correlation', 'matrix', 'definite', 'correlation', 'matrix', 'which', 'brakes', 'var', 'engine', 'correlation', 'matrix', 'for', 'any', 'set', 'of', 'random', 'variables', 'is', 'non', 'negative', 'bydefinition', 'and', 'remains', 'non', 'negatively', 'definite', 'if', 'calculated', 'properly', 'basedon', 'any', 'historical', 'data', 'here', 'according', 'to', 'our', 'phone', 'discussion', 'we', 'started', 'experimenting', 'assuming', 'the', 'same', 'correlation', 'for', 'each', 'efa', 'slot', 'and', 'et', 'elecversus', 'gas', 'am', 'sending', 'you', 'the', 'spreadsheet', 'which', 'summaries', 'the', 'results', 'inaddition', 'to', 'the', 'aggregate', 'var', 'numbers', 'for', 'the', 'runs', 'oliver', 'did', 'you', 'can', 'seethe', 'var', 'numbers', 'based', 'on', 'correlation', 'matrix', 'and', 'matrix', 'in', 'matrix', 'thecorrelations', 'across', 'efa', 'slots', 'are', 'identical', 'to', 'these', 'in', 'original', 'matrix', 'obtained', 'this', 'matrix', 'by', 'trial', 'and', 'error', 'matrix', 'is', 'produces', 'by', 'naveenusing', 'finger', 'algorithm', 'it', 'differs', 'from', 'original', 'matrix', 'across', 'efa', 'slots', 'aswellas', 'in', 'power', 'versus', 'gas', 'correlations', 'and', 'gives', 'higher', 'var', 'than', 'matrix', 'does', 'calculate', 'historical', 'correlations', 'from', 'them', 'tanya', 'oliver', 'gaylardleppard', 'lon', 'ect', 'ect', 'rudy', 'dautel', 'hou', 'ect', 'ect', 'kirstee', 'hewitt', 'lon', 'ect', 'ect', 'naveen', 'andrews', 'corp', 'enron', 'enron', 'tanya', 'tamarchenko', 'hou', 'ect', 'ect', 'davidport', 'market', 'risk', 'corp', 'enron', 'var', 'uk', 'power', 'book', 'var', 'uk', 'gas', 'book', 'mm', 'mm', 'mm', 'mm', 'mm', 'mm', 'mm', 'mm', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'cholesky', 'decomposition', 'failed', 'not', 'positive', 'definite', 'peaks', 'and', 'off', 'peaks', 'were', 'treated', 'the', 'same', 'to', 'avoid', 'violating', 'the', 'matrix', 'sintegrity', 'interesting', 'to', 'note', 'that', 'for', 'higher', 'correlation', 'of', 'the', 'power', 'varincreases', 'which', 'is', 'counter', 'to', 'intuition', 'this', 'implies', 'that', 'we', 'need', 'to', 'lookinto', 'how', 'the', 'correlations', 'are', 'being', 'applied', 'within', 'the', 'model', 'once', 'we', 'canderive', 'single', 'correlations', 'from', 'the', 'term', 'structure', 'is', 'the', 'next', 'action', 'tounderstand', 'how', 'they', 'are', 'being', 'applied', 'and', 'whether', 'the', 'model', 'captures', 'the', 'lvolatility', 'in', 'the', 'spread', 'option', 'deals', 'from', 'onwards', 'the', 'var', 'calculation', 'failed', 'oliver']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_image_image', 'getting', 'together', 'for', 'the', 'holidays', 'is', 'something', 'we', 'all', 'enjoy', 'whether', 'it', 'gathering', 'with', 'old', 'friends', 'from', 'out', 'of', 'town', 'or', 'hanging', 'out', 'with', 'the', 'usual', 'gang', 'every', 'gathering', 'this', 'time', 'of', 'year', 'seems', 'little', 'more', 'special', 'we', 'at', 'miller_brewing', 'wish', 'you', 'many', 'happy', 'celebrations', 'this', 'season', 'and', 'thank', 'you', 'for', 'enjoying', 'those', 'occasions', 'responsibly', 'happy_holidays', 'image', 'this', 'mail', 'is', 'not', 'sent', 'unsolicited', 'you', 'subscribed', 'to', 'receive', 'information', 'from', 'miller_brewing', 'at', 'miller', 'web_site', 'or', 'event', 'must', 'be', 'or', 'older', 'to', 'visit', 'our', 'web_site', 'miller_brewing', 'co', 'milwaukee', 'wi', 'miller_brewing', 'company', 'milwaukee', 'wi', 'privacy_statement', 'image_image_image_image', 'this', 'message', 'was', 'sent', 'by', 'miller_brewing', 'company', 'using', 'responsys', 'interact', 'tm', 'click_here', 'if', 'you', 'prefer', 'not', 'to', 'receive', 'future', 'mail', 'from', 'miller_brewing', 'company', 'click_here', 'to', 'view', 'our', 'permission', 'marketing', 'policy', 'image']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[200]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop_words, make bigrams and lemmatize\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_image', 'image', 'get', 'together', 'holiday', 'something', 'enjoy', 'gather', 'old', 'friend', 'town', 'hang', 'usual', 'gang', 'gathering', 'time', 'year', 'seem', 'little', 'special', 'miller_brew', 'wish', 'many', 'happy', 'celebration', 'season', 'thank', 'enjoy', 'occasion', 'responsibly', 'happy_holiday', 'image', 'mail', 'send', 'unsolicited', 'subscribed', 'receive', 'information', 'miller_brew', 'miller', 'web_site', 'event', 'must', 'old', 'visit', 'web_site', 'miller_brew', 'co', 'milwaukee', 'wi', 'miller_brew', 'company', 'milwaukee', 'wi', 'privacy', 'statement', 'image_image', 'image_image', 'message', 'send', 'miller_brew', 'company', 'use', 'responsy', 'interact', 'tm', 'click', 'prefer', 'receive', 'future', 'mail', 'miller_brew', 'company', 'click', 'view', 'permission', 'marketing', 'policy', 'image']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary and corpus both are needed for (LDA) topic modeling\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = './mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling\n",
    "# corpus, dictionary and number of topics required for LDA\n",
    "# alpha and eta are hyperparameters that affect sparsity of the topics\n",
    "# chunksize is the number of documents to be used in each training chunk\n",
    "# update_every determines how often the model parameters should be updated\n",
    "# passes is the total number of training passes\n",
    "# Print the Keyword in the 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.print_topics())# The weights reflect how important a keyword is to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model perplexity and topic coherence provide a convenient\n",
    "# measure to judge how good a given topic model is.\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook(sort=True)\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "print(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the topics with mallet model\n",
    "# pyLDAvis.enable_notebook(sort=True)\n",
    "# vis = pyLDAvis.gensim.prepare(ldamallet, corpus, id2word)\n",
    "# pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[4]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.Keywords.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.Text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
